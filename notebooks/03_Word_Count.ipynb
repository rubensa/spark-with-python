{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "\n",
    "### Counting the number of occurances of words in a text is a popular first exercise using map-reduce.\n",
    "\n",
    "## The Task\n",
    "**Input:** A text file consisisting of words separated by spaces.  \n",
    "**Output:** A list of words and their counts, sorted from the most to the least common.\n",
    "\n",
    "We will use the book \"Moby Dick\" as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/sdkman/candidates/java/current\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup a plan for pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use `textFile()` to read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 1.51 ms, sys: 683 µs, total: 2.19 ms\nWall time: 653 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(\"data/moby-dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "pyspark.rdd.RDD"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps for counting the words\n",
    "\n",
    "* split line by spaces.\n",
    "* map `word` to `(word,1)`\n",
    "* count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 9.26 ms, sys: 4.61 ms, total: 13.9 ms\nWall time: 86.3 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "not_empty = words.filter(lambda x: x!='') \n",
    "key_values= not_empty.map(lambda word: (word, 1)) \n",
    "counts=     key_values.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### flatMap()\n",
    "Note the line:\n",
    "```python\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "Why are we using `flatMap`, rather than `map`?\n",
    "\n",
    "The reason is that the operation `line.split(\" \")` generates a **list** of strings, so had we used `map` the result would be an RDD of lists of words. Not an RDD of words.\n",
    "\n",
    "The difference between `map` and `flatMap` is that the second expects to get a list as the result from the map and it **concatenates** the lists to form the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The execution plan\n",
    "In the last cell we defined the execution plan, but we have not started to execute it.\n",
    "\n",
    "* Preparing the plan took ~100ms, which is a non-trivial amount of time, \n",
    "* But much less than the time it will take to execute it.\n",
    "* Lets have a look a the execution plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding the details\n",
    "To see which step in the plan corresponds to which RDD we print out the execution plan for each of the RDDs.  \n",
    "\n",
    "Note that the execution plan for `words`, `not_empty` and `key_values` are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) data/moby-dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  data/moby-dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
    }
   ],
   "source": [
    "pretty_print_plan(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[6] at RDD at PythonRDD.scala:53 []\n |  data/moby-dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  data/moby-dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
    }
   ],
   "source": [
    "pretty_print_plan(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[7] at RDD at PythonRDD.scala:53 []\n |  data/moby-dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  data/moby-dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
    }
   ],
   "source": [
    "pretty_print_plan(not_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[8] at RDD at PythonRDD.scala:53 []\n |  data/moby-dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n |  data/moby-dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
    }
   ],
   "source": [
    "pretty_print_plan(key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[9] at RDD at PythonRDD.scala:53 []\n |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:133 []\n |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []\n +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:4 []\n    |  PythonRDD[2] at reduceByKey at <timed exec>:4 []\n    |  data/moby-dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n    |  data/moby-dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
    }
   ],
   "source": [
    "pretty_print_plan(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "| Execution plan   | RDD |  Comments |\n",
    "| :---------------------------------------------------------------- | :------------: | :--- |\n",
    "|`(2)_PythonRDD[9] at RDD at PythonRDD.scala:53 []`| **counts** | Final RDD|\n",
    "|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:133 []`| **---\"---** |\n",
    "|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** | RDD is partitioned by key |\n",
    "|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** | Perform mapByKey |\n",
    "|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** | The result of  partitioning into words|\n",
    "| | |  removing empties, and making into (word,1) pairs|\n",
    "|`____/__../../data/moby-dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** | The partitioned text |\n",
    "|`____/__../../data/moby-dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** | The text source |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execution\n",
    "Finally we count the number of times each word has occured.\n",
    "Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 20.2 ms, sys: 0 ns, total: 20.2 ms\nWall time: 1.7 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "## Run #1\n",
    "Count=counts.count()  # Count = the number of different words\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # \n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Amortization\n",
    "When the same commands are performed repeatedly on the same data, the execution time tends to decrease in later executions.\n",
    "\n",
    "The cells below are identical to the one above, with one exception at `Run #3`\n",
    "\n",
    "Observe that `Run #2` take much less time that `Run #1`. Even though no `cache()` was explicitly requested. The reason is that Spark caches (or materializes) `key_values`, before executing `reduceByKey()` because performng reduceByKey requires a shuffle, and a shuffle requires that the input RDD is materialized. In other words, sometime caching happens even if the programmer did not ask for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 17 ms, sys: 3.55 ms, total: 20.6 ms\nWall time: 208 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "## Run #2\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explicit Caching\n",
    "In `Run #3` we explicitly ask for `counts` to be cached. This will reduce the execution time in the following run `Run #4` by a little bit, but not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 16.5 ms, sys: 0 ns, total: 16.5 ms\nWall time: 207 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "## Run #3, cache\n",
    "Count=counts.cache().count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 16.6 ms, sys: 0 ns, total: 16.6 ms\nWall time: 191 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Run #4\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 14.4 ms, sys: 474 µs, total: 14.9 ms\nWall time: 118 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Run #5\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}