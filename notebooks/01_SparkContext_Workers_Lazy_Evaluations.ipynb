{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext - number of workers and lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the impact of number of workers\n",
    "While initializing the `SparkContext`, we can specify number of worker nodes. Generally, it is recommended to have one worker per core of the machine. But it can be smaller or larger. In the following code, we will examine the impact of number of worker cores on some parallelized operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1 executors, time = 1.6730988025665283\n2 executors, time = 1.1461057662963867\n3 executors, time = 1.0394089221954346\n4 executors, time = 1.0736877918243408\n"
    }
   ],
   "source": [
    "for j in range(1,5):\n",
    "    sc= SparkContext(master = \"local[%d]\"%(j))\n",
    "    t0=time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2]*10000).reduce(lambda x,y:x+y)\n",
    "    print(f\"{j} executors, time = {time()-t0}\")\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We observe that it takes almost double time for 1 worker, and after that time reduces to a flat level for 2,3,4 workers etc. This is because this code run on a Linux virtual box using only 2 cores from the host machine. If you run this code on a machine with 4 cores, you will see benefit upto 4 cores and then the flattening out of the time taken. It also become clear that using more than one worker per core is not beneficial as it just does context-switching in that case and does not speed up the parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Showing the essence of _lazy_ evaluation\n",
    "![](https://raw.githubusercontent.com/rubensa/spark-with-python/master/images/lazy.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a RDD with 1 million elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 0 ns, sys: 1.52 ms, total: 1.52 ms\nWall time: 3.9 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "rdd1 = sc.parallelize(range(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some computing function - `taketime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "def taketime(x):\n",
    "    [cos(j) for j in range(100)]\n",
    "    return cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check how much time is taken by `taketime` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 202 µs, sys: 66 µs, total: 268 µs\nWall time: 17.1 ms\n"
    },
    {
     "data": {
      "text/plain": "-0.4161468365471424"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taketime(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now do the `map` operation on the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 17 µs, sys: 6 µs, total: 23 µs\nWall time: 27.4 µs\n"
    }
   ],
   "source": [
    "%%time\n",
    "interim = rdd1.map(lambda x: taketime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### How come each taketime function takes 45.8 us but the map operation with a 10000 element RDD also took similar time?<br><br>Because of _lazy_ evaluation i.e. nothing was computed in the previous step, just a plan of execution was made. The variable `interim` does not point to a data structure, instead it points to a plan of execution, expressed as a dependency graph. The dependency graph defines how RDDs are computed from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's see the \"Dependency Graph\" using `toDebugString` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[1] at RDD at PythonRDD.scala:52 []\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:194 []\n"
    }
   ],
   "source": [
    "print(interim.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "![](https://raw.githubusercontent.com/rubensa/spark-with-python/master/images/rdd_dependency_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The actual execution by `reduce` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "output = -0.28870546796843666\nCPU times: user 12.1 ms, sys: 0 ns, total: 12.1 ms\nWall time: 9.99 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "print('output =',interim.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "31.0"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000000*31e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### It is less than what we would have expected considering 1 million operations with the `taketime` function. This is the result of parallel operation of 2 cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, we have not saved (materialized) any intermediate results in `interim`, so another simple operation (e.g. counting elements > 0) will take almost same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "500000\nCPU times: user 1.19 ms, sys: 9.74 ms, total: 10.9 ms\nWall time: 9.33 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "print(interim.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Caching to reduce computation time on similar operation (spending memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the same computation as before with `cache` method to tell the dependency graph to plan for caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CPU times: user 7.04 ms, sys: 773 µs, total: 7.82 ms\nWall time: 17.8 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "interim = rdd1.map(lambda x: taketime(x)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2) PythonRDD[4] at RDD at PythonRDD.scala:52 [Memory Serialized 1x Replicated]\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:194 [Memory Serialized 1x Replicated]\n"
    }
   ],
   "source": [
    "print(interim.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "output = -0.28870546796843666\nCPU times: user 7.67 ms, sys: 0 ns, total: 7.67 ms\nWall time: 9.34 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "print('output =',interim.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now run the same `filter` method with the help of cached result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "500000\nCPU times: user 7.07 ms, sys: 2.28 ms, total: 9.35 ms\nWall time: 317 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "print(interim.filter(lambda x:x>0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This time it took much shorter time due to cached result, which it could use to compare to 0 and count easily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}